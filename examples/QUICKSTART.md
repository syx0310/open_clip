# Quick Start: DinoV3 + Qwen Training

## ğŸš€ ä¸€é”®è¿è¡Œ

```bash
cd /home/user/open_clip
bash examples/setup_and_run.sh
```

è¿™ä¸ªè„šæœ¬ä¼šè‡ªåŠ¨ï¼š
1. æ£€æŸ¥Pythonç¯å¢ƒ
2. å®‰è£…æ‰€æœ‰ä¾èµ–
3. æä¾›äº¤äº’å¼èœå•é€‰æ‹©è®­ç»ƒæ¨¡å¼

---

## ğŸ“‹ æ‰‹åŠ¨å®‰è£…å’Œè¿è¡Œ

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /home/user/open_clip

# å®‰è£…PyTorch (CUDA 11.8)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…Transformerså’Œå…¶ä»–ä¾èµ–
pip install transformers accelerate pillow

# å®‰è£…OpenCLIP
pip install -e .
```

### æ­¥éª¤2: éªŒè¯å®‰è£…

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
```

### æ­¥éª¤3: è¿è¡Œè®­ç»ƒ

#### é€‰é¡¹A: 1:1 è®­ç»ƒ (æ ‡å‡†CLIP) â­ æ¨è

```bash
python examples/train_dinov3_qwen.py \
    --mode 1to1 \
    --vision-model facebook/dinov2-base \
    --text-model Qwen/Qwen2-0.5B \
    --batch-size 16 \
    --epochs 10 \
    --learning-rate 1e-4
```

#### é€‰é¡¹B: 1:N è®­ç»ƒ (1ä¸ªæ–‡æœ¬ç¼–ç å™¨ + Nä¸ªè§†è§‰ç¼–ç å™¨)

```bash
python examples/train_dinov3_qwen.py \
    --mode 1text_nvision \
    --num-vision 3 \
    --vision-model facebook/dinov2-base \
    --text-model Qwen/Qwen2-0.5B \
    --batch-size 8 \
    --epochs 10 \
    --aggregation mean
```

#### é€‰é¡¹C: N:1 è®­ç»ƒ (Nä¸ªæ–‡æœ¬ç¼–ç å™¨ + 1ä¸ªè§†è§‰ç¼–ç å™¨)

```bash
python examples/train_dinov3_qwen.py \
    --mode ntext_1vision \
    --num-text 2 \
    --vision-model facebook/dinov2-base \
    --text-model Qwen/Qwen2-0.5B \
    --batch-size 8 \
    --epochs 10 \
    --aggregation mean
```

---

## ğŸ¯ ä½¿ç”¨çœŸå®çš„DinoV3å’ŒQwenæ¨¡å‹

### DinoV3è§†è§‰æ¨¡å‹

```bash
# ä½¿ç”¨DinoV3-ViT-H/16+ (æ¨èç”¨äºç”Ÿäº§)
python examples/train_dinov3_qwen.py \
    --vision-model facebook/dinov2-giant \
    --text-model Qwen/Qwen2-7B \
    --embed-dim 1536 \
    --batch-size 4
```

å¯ç”¨çš„DinoV2/V3æ¨¡å‹:
- `facebook/dinov2-small` - 22Må‚æ•°
- `facebook/dinov2-base` - 86Må‚æ•° (æ¨èæµ‹è¯•ç”¨)
- `facebook/dinov2-large` - 304Må‚æ•°
- `facebook/dinov2-giant` - 1.1Bå‚æ•° (æœ€å¼ºæ€§èƒ½)

### Qwenæ–‡æœ¬æ¨¡å‹

```bash
# ä½¿ç”¨Qwen2-7Bæˆ–Qwen3-Embedding-4B
python examples/train_dinov3_qwen.py \
    --text-model Qwen/Qwen2-7B \
    --embed-dim 2048
```

å¯ç”¨çš„Qwenæ¨¡å‹:
- `Qwen/Qwen2-0.5B` - 0.5Bå‚æ•° (æ¨èæµ‹è¯•ç”¨)
- `Qwen/Qwen2-1.5B` - 1.5Bå‚æ•°
- `Qwen/Qwen2-7B` - 7Bå‚æ•° (æ¨èç”Ÿäº§ç”¨)

---

## ğŸ“Š å†…å­˜éœ€æ±‚å’Œæ‰¹é‡å¤§å°å»ºè®®

| é…ç½® | GPUå†…å­˜ | æ‰¹é‡å¤§å° | è®­ç»ƒæ—¶é—´(1000æ­¥) |
|------|---------|---------|------------------|
| DinoV2-Base + Qwen2-0.5B | 8GB | 32 | ~20åˆ†é’Ÿ |
| DinoV2-Large + Qwen2-1.5B | 16GB | 16 | ~40åˆ†é’Ÿ |
| DinoV2-Giant + Qwen2-7B | 40GB | 4 | ~2å°æ—¶ |

---

## ğŸ”§ å¸¸è§å‚æ•°è¯´æ˜

```bash
python examples/train_dinov3_qwen.py \
    --mode 1to1                          # è®­ç»ƒæ¨¡å¼: 1to1, 1text_nvision, ntext_1vision
    --vision-model facebook/dinov2-base  # HuggingFaceè§†è§‰æ¨¡å‹
    --text-model Qwen/Qwen2-0.5B        # HuggingFaceæ–‡æœ¬æ¨¡å‹
    --embed-dim 768                      # åµŒå…¥ç»´åº¦
    --batch-size 16                      # æ‰¹é‡å¤§å°
    --epochs 10                          # è®­ç»ƒè½®æ•°
    --learning-rate 1e-4                 # å­¦ä¹ ç‡
    --num-samples 1000                   # æ ·æœ¬æ•°(ä»…dummyæ•°æ®)
    --device cuda                        # è®¾å¤‡: cudaæˆ–cpu
    --checkpoint-dir ./checkpoints       # æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
    --log-interval 10                    # æ—¥å¿—è¾“å‡ºé—´éš”
```

æŸ¥çœ‹æ‰€æœ‰å‚æ•°:
```bash
python examples/train_dinov3_qwen.py --help
```

---

## ğŸ› æ•…éšœæ’é™¤

### å†…å­˜ä¸è¶³ (Out of Memory)

```bash
# å‡å°æ‰¹é‡å¤§å°
python examples/train_dinov3_qwen.py --batch-size 8

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
python examples/train_dinov3_qwen.py \
    --vision-model facebook/dinov2-small \
    --text-model Qwen/Qwen2-0.5B
```

### CUDAä¸å¯ç”¨

```bash
# ä½¿ç”¨CPU (è¾ƒæ…¢)
python examples/train_dinov3_qwen.py --device cpu --batch-size 4
```

### æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# è®¾ç½®HuggingFaceé•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
python examples/train_dinov3_qwen.py \
    --vision-model /path/to/local/dinov2 \
    --text-model /path/to/local/qwen
```

---

## ğŸ“ æ›¿æ¢ä¸ºçœŸå®æ•°æ®

ç¼–è¾‘ `train_dinov3_qwen.py` ä¸­çš„ `DummyImageTextDataset` ç±»:

```python
class YourImageTextDataset(Dataset):
    def __init__(self, image_dir, text_file, tokenizer, image_processor):
        self.images = load_image_paths(image_dir)
        self.texts = load_texts(text_file)
        self.tokenizer = tokenizer
        self.image_processor = image_processor

    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image = Image.open(self.images[idx]).convert('RGB')
        image = self.image_processor(image, return_tensors='pt')['pixel_values'][0]

        # æ ‡è®°åŒ–æ–‡æœ¬
        text = self.tokenizer(
            self.texts[idx],
            max_length=77,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )['input_ids'][0]

        return image, text
```

---

## ğŸ“ è®­ç»ƒè¾“å‡ºç¤ºä¾‹

```
========================================================================
CLIP Training: DinoV3 + Qwen Embedding
========================================================================
Mode: 1to1
Vision model: facebook/dinov2-base
Text model: Qwen/Qwen2-0.5B
Embedding dimension: 768
Device: cuda
Batch size: 16
Learning rate: 0.0001
========================================================================

[1/5] Creating model...
âœ“ Model created with 124,567,890 parameters

[2/5] Creating dataset...
Created dummy dataset with 1000 samples
âš ï¸  IMPORTANT: Replace this with your actual image-text dataset!
âœ“ Dataset created with 1000 samples

[3/5] Creating optimizer...
âœ“ Total training steps: 625

[4/5] Training...
--------------------------------------------------------------------------------
Epoch 1 [0/62] Loss: 6.2145 Logit Scale: 14.27 LR: 0.000100
Epoch 1 [10/62] Loss: 5.8932 Logit Scale: 14.45 LR: 0.000099
...
âœ“ Epoch 1/10 completed | Avg Loss: 5.4521
...
âœ“ Checkpoint saved: ./checkpoints/checkpoint_epoch_5.pt
...
âœ“ Epoch 10/10 completed | Avg Loss: 2.1234
--------------------------------------------------------------------------------

[5/5] Saving final model...
âœ“ Checkpoint saved: ./checkpoints/checkpoint_epoch_10.pt

[Bonus] Testing inference...
âœ“ Similarity matrix (diagonal should be high):
  [0.8234, 0.7891, 0.8456, 0.7623]

========================================================================
âœ… TRAINING COMPLETED SUCCESSFULLY!
Checkpoints saved to: ./checkpoints
========================================================================
```

---

## ğŸ“š æ›´å¤šä¿¡æ¯

- å®Œæ•´æ–‡æ¡£: [REFACTORING_GUIDE.md](../REFACTORING_GUIDE.md)
- è¯¦ç»†è¯´æ˜: [README_DINOV3_QWEN.md](README_DINOV3_QWEN.md)
- ç¤ºä¾‹ä»£ç : [train_dinov3_qwen.py](train_dinov3_qwen.py)

---

## ğŸ’¡ ä¸‹ä¸€æ­¥

1. âœ… è¿è¡Œæµ‹è¯•è®­ç»ƒéªŒè¯ç¯å¢ƒ
2. ğŸ“Š å‡†å¤‡ä½ çš„å›¾åƒ-æ–‡æœ¬æ•°æ®é›†
3. ğŸ”§ è°ƒæ•´è¶…å‚æ•°ä¼˜åŒ–æ€§èƒ½
4. ğŸš€ æ‰©å±•åˆ°åˆ†å¸ƒå¼å¤šGPUè®­ç»ƒ
5. ğŸ“ˆ æ·»åŠ éªŒè¯å’Œè¯„ä¼°æŒ‡æ ‡
