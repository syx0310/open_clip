# å®Œæ•´å‘½ä»¤åˆ—è¡¨ï¼šä»å®‰è£…åˆ°æ‰§è¡Œ

## ğŸ“¦ æ–¹æ³•1: ä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰

```bash
cd /home/user/open_clip
bash examples/setup_and_run.sh
```

æŒ‰ç…§äº¤äº’å¼æç¤ºé€‰æ‹©å³å¯ï¼

---

## ğŸ”§ æ–¹æ³•2: æ‰‹åŠ¨å®‰è£…å’Œè¿è¡Œ

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /home/user/open_clip

# å®‰è£…PyTorch (æ ¹æ®ä½ çš„CUDAç‰ˆæœ¬é€‰æ‹©)
# CUDA 11.8:
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1:
# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# CPU only:
# pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# å®‰è£…Transformerså’Œå…¶ä»–ä¾èµ–
pip install transformers accelerate pillow

# å®‰è£…OpenCLIP (å¼€å‘æ¨¡å¼)
pip install -e .
```

### æ­¥éª¤2: éªŒè¯å®‰è£…

```bash
python -c "import torch; print(f'âœ“ PyTorch {torch.__version__}')"
python -c "import transformers; print(f'âœ“ Transformers {transformers.__version__}')"
python -c "import torch; print(f'âœ“ CUDA: {torch.cuda.is_available()}')"
python -c "from open_clip.base_model import TransformersVisionEncoder; print('âœ“ OpenCLIP OK')"
```

### æ­¥éª¤3: è¿è¡Œè®­ç»ƒ

#### ğŸ¯ 1:1 æ ‡å‡†è®­ç»ƒï¼ˆDinoV2 + Qwenï¼‰

```bash
python examples/train_dinov3_qwen.py \
    --mode 1to1 \
    --vision-model facebook/dinov2-base \
    --text-model Qwen/Qwen2-0.5B \
    --embed-dim 768 \
    --batch-size 16 \
    --epochs 10 \
    --learning-rate 1e-4 \
    --num-samples 1000 \
    --device cuda
```

#### ğŸ”„ 1:N è®­ç»ƒï¼ˆ1ä¸ªæ–‡æœ¬ + å¤šä¸ªè§†è§‰ç¼–ç å™¨ï¼‰

```bash
python examples/train_dinov3_qwen.py \
    --mode 1text_nvision \
    --num-vision 3 \
    --vision-model facebook/dinov2-base \
    --text-model Qwen/Qwen2-0.5B \
    --embed-dim 768 \
    --batch-size 8 \
    --epochs 10 \
    --aggregation mean \
    --device cuda
```

#### ğŸ” N:1 è®­ç»ƒï¼ˆå¤šä¸ªæ–‡æœ¬ç¼–ç å™¨ + 1ä¸ªè§†è§‰ï¼‰

```bash
python examples/train_dinov3_qwen.py \
    --mode ntext_1vision \
    --num-text 2 \
    --vision-model facebook/dinov2-base \
    --text-model Qwen/Qwen2-0.5B \
    --embed-dim 768 \
    --batch-size 8 \
    --epochs 10 \
    --aggregation mean \
    --device cuda
```

---

## ğŸš€ ä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒ

### DinoV2-Giant + Qwen2-7B (éœ€è¦40GB+ GPU)

```bash
python examples/train_dinov3_qwen.py \
    --mode 1to1 \
    --vision-model facebook/dinov2-giant \
    --text-model Qwen/Qwen2-7B \
    --embed-dim 1536 \
    --batch-size 4 \
    --epochs 10 \
    --learning-rate 5e-5 \
    --device cuda
```

---

## ğŸ§ª å¿«é€Ÿæµ‹è¯•ï¼ˆå°æ•°æ®é›†ï¼‰

```bash
python examples/train_dinov3_qwen.py \
    --mode 1to1 \
    --vision-model facebook/dinov2-small \
    --text-model Qwen/Qwen2-0.5B \
    --batch-size 32 \
    --epochs 3 \
    --num-samples 100 \
    --log-interval 2
```

---

## ğŸ“Š æŸ¥çœ‹è®­ç»ƒç»“æœ

```bash
# æŸ¥çœ‹æ£€æŸ¥ç‚¹
ls -lh checkpoints/

# åŠ è½½å’Œæµ‹è¯•æ£€æŸ¥ç‚¹
python -c "
import torch
checkpoint = torch.load('checkpoints/checkpoint_epoch_10.pt')
print(f'Epoch: {checkpoint[\"epoch\"]}')
print(f'Loss: {checkpoint[\"loss\"]:.4f}')
"
```

---

## ğŸ” æ‰€æœ‰å¯ç”¨å‚æ•°

```bash
python examples/train_dinov3_qwen.py --help
```

è¾“å‡ºï¼š
```
usage: train_dinov3_qwen.py [-h]
    [--mode {1to1,1text_nvision,ntext_1vision}]
    [--num-vision NUM_VISION]
    [--num-text NUM_TEXT]
    [--vision-model VISION_MODEL]
    [--text-model TEXT_MODEL]
    [--embed-dim EMBED_DIM]
    [--vision-pooler {cls,mean,max}]
    [--text-pooler {cls,mean,max}]
    [--batch-size BATCH_SIZE]
    [--epochs EPOCHS]
    [--learning-rate LEARNING_RATE]
    [--weight-decay WEIGHT_DECAY]
    [--max-grad-norm MAX_GRAD_NORM]
    [--num-samples NUM_SAMPLES]
    [--image-size IMAGE_SIZE]
    [--max-text-length MAX_TEXT_LENGTH]
    [--num-workers NUM_WORKERS]
    [--device DEVICE]
    [--checkpoint-dir CHECKPOINT_DIR]
    [--log-interval LOG_INTERVAL]
    [--save-interval SAVE_INTERVAL]
    [--aggregation {mean,max,weighted}]
```

---

## ğŸ¨ å¯ç”¨æ¨¡å‹åˆ—è¡¨

### Vision Models (HuggingFace)
```bash
--vision-model facebook/dinov2-small       # 22M params
--vision-model facebook/dinov2-base        # 86M params (æ¨è)
--vision-model facebook/dinov2-large       # 304M params
--vision-model facebook/dinov2-giant       # 1.1B params
--vision-model google/vit-base-patch16-224 # ViT-B/16
--vision-model microsoft/swin-base-patch4-window7-224
```

### Text Models (HuggingFace)
```bash
--text-model Qwen/Qwen2-0.5B              # 0.5B params (æ¨èæµ‹è¯•)
--text-model Qwen/Qwen2-1.5B              # 1.5B params
--text-model Qwen/Qwen2-7B                # 7B params (æ¨èç”Ÿäº§)
--text-model bert-base-uncased             # BERT-Base
--text-model roberta-base                  # RoBERTa-Base
```

---

## ğŸ› å¸¸è§é—®é¢˜è§£å†³

### 1. å†…å­˜ä¸è¶³ (OOM)

```bash
# å‡å°æ‰¹é‡å¤§å°
python examples/train_dinov3_qwen.py --batch-size 4

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
python examples/train_dinov3_qwen.py \
    --vision-model facebook/dinov2-small \
    --text-model Qwen/Qwen2-0.5B
```

### 2. CUDAä¸å¯ç”¨

```bash
# ä½¿ç”¨CPU
python examples/train_dinov3_qwen.py --device cpu --batch-size 4
```

### 3. æ¨¡å‹ä¸‹è½½æ…¢

```bash
# ä½¿ç”¨HuggingFaceé•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
python examples/train_dinov3_qwen.py ...
```

### 4. æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ

```bash
# ç›‘æ§GPU
watch -n 1 nvidia-smi

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œè®­ç»ƒ
python examples/train_dinov3_qwen.py ...
```

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. **æ›¿æ¢ä¸ºçœŸå®æ•°æ®**ï¼šç¼–è¾‘ `train_dinov3_qwen.py` ä¸­çš„ `DummyImageTextDataset`
2. **è°ƒæ•´è¶…å‚æ•°**ï¼šæ ¹æ®ä½ çš„æ•°æ®é›†å¤§å°å’ŒGPUå†…å­˜è°ƒæ•´
3. **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šä½¿ç”¨torchrunè¿›è¡Œå¤šGPUè®­ç»ƒ
4. **è¯„ä¼°æŒ‡æ ‡**ï¼šæ·»åŠ éªŒè¯é›†å’Œè¯„ä¼°æŒ‡æ ‡

---

## ğŸ“š å®Œæ•´æ–‡æ¡£

- å¿«é€Ÿå¼€å§‹: [examples/QUICKSTART.md](examples/QUICKSTART.md)
- è¯¦ç»†æŒ‡å—: [examples/README_DINOV3_QWEN.md](examples/README_DINOV3_QWEN.md)
- é‡æ„æŒ‡å—: [REFACTORING_GUIDE.md](REFACTORING_GUIDE.md)

---

## âœ… éªŒè¯ä¸€åˆ‡æ­£å¸¸

è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼š

```bash
python examples/test_dinov3_qwen.py
```

è¿™ä¼šæµ‹è¯•æ‰€æœ‰ä¸‰ç§æ¨¡å¼ï¼ˆ1:1, 1:N, N:1ï¼‰å¹¶éªŒè¯æ‰€æœ‰åŠŸèƒ½ã€‚

---

**å°±æ˜¯è¿™æ ·ï¼äº«å—è®­ç»ƒå§ï¼ğŸ‰**
